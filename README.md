# 深度学习入门 2：自制框架笔记

## 实现效果
<img width="2490" height="1730" alt="0" src="https://github.com/user-attachments/assets/1a7434db-e295-494e-b4f2-bfcabbf93667" />


## 技术细节

### 自动微分

#### 变量
- `Variable`类是一个存放数据的箱子，不是数据。
- 其中使用了`numpy.array`来保存数据。

#### 函数
- `Function`类
  - 实现`__call__`和`forward`两种方法。
  - `__call__`方法用来初始化，`forward`方法用来执行具体逻辑。

#### 正向传播与反向传播
- 变量/函数两者具有明显的对应关系
- 变量有普通值和导数值，函数有普通计算(正向传播)和求导计算(反向传播)

#### 手动反向传播
- 在`backward`函数中，实现函数主体的求导，并根据输入值得到输出值。

#### 自动反向传播
- 无论正向传播是怎么样计算的，反向传播都能自动进行。
- 函数的角度看待变量：变量通过输入输出形式存在
- 变量的角度看待函数：变量是由函数的创造的，函数是变量父母

##### 实现方法
- 记录创建变量的函数
- 在变量中添加一个反向传播的方法
- 该方法中实现递归调用反向传播

##### 递归到循环
- 添加一个栈，将调用方式从`pop`循环调用变为递归调用

#### 自然代码表达

##### 支持可变长参数
- 列表存储可变长的参数
- 添加`*`号，解包方式输入参数
- 非元组变元组
- 反向传播也通过列表的方式

##### 重复使用一个变量
- 判断该变量的导数是否存在，存在则相加，不存在则直接赋值。

##### 复杂计算图顺序
- 采用拓扑排序
- 记录变量生成的辈分关系按优先级取出

##### 记录辈分并排序
- 记录辈分后，按照辈分排序。比较耗时。

##### 循环引用问题
- 通过`weakref`弱引用来解决

##### 内存优化点
- 不保留中间变量
- 判断当前属于训练还是推理阶段

##### 高阶导数
- 将变量梯度设置为变量这种格式。
- 同时，在计算时及时清空梯度。

### 神经网络创建

#### 改变形状的函数反向传播
- 记录函数的改变的形状，并在反向传播时生效。

#### 压缩维度函数的反向传播
- 诸如`sum`这种函数，会先使用维度恢复函数将对应的维度恢复；
- 之后，使用`broadcast`函数将梯度填充到对应的维度中。

#### 直接函数 vs 类`forward`
- 直接使用函数有中间变量
- 使用类定义的函数没有中间变量，更加节省空间

#### 参数自动化管理
- `parameter`类：参数收到`parameter`类中
- `layer`类：`parameter`集合到`layer`类中，`layer`类同时可以囊括其他的`layer`类
- `linear`类：`layer`集合到`linear`类中

#### 优化器优化参数
- `Optimizers`类是实现优化器操作的基类。
- 在`model`计算完后，会通过`Optimizers`对`model`的参数进行优化。

#### Inverted Dropout
- 在训练时就对结果进行缩放，最终期望相同，可以不在测试时再改变。
